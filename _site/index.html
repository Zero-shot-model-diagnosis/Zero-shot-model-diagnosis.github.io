<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>ZOOM: Zero-shot Model Diagnosis</title>
<link href="./ZOOM_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./ZOOM_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./ZOOM_files/jquery.js"></script>
</head>

<body>
<div class="content">
  <br>
  <br>
  <br>
  <h1><strong>ZOOM: Zero-shot Model Diagnosis </strong></h1>
  <p id="authors"><span><a href="https://natanielruiz.github.io/"></a></span>
    <a href="https://peterljq.github.io/">Jinqi Luo*</a> 
    <a href="https://www.zhaoningwang.com">Zhaoning Wang*</a> 
    <a href="https://github.com/ChenWu98">Chen Henry Wu</a> 
    <a href="https://www.donghuang-research.com/">Dong Huang</a> 
    <a href="https://www.cs.cmu.edu/~ftorre/">Fernando De La Torre</a> 
    
    <br>
    <br>
    <br>

    <br>
    <br>

  <span style="font-size: 24px">Carnegie Mellon University
  </span></p>
  <br>
  <br>
  <br>
  <font size="+2">
    <p style="text-align: center;">
      <a href="" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
      <a href="./ZOOM_files/bibtex.txt" target="_blank">[BibTeX]</a>
    </p>
</font>
  <br>

</div>
<div class="content">
  <br>
  <img src="./ZOOM_files/teaser_figure_v5.jpg" class="teaser-gif" style="width:100%;"><br>
  <hr>
  <h2 style="text-align:center"><b><em>Diagnose your model with a generative model, just provide the attributes and our pipeline will give their sensitivity analysis.</em></b></h3>

</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>When it comes to deploying deep vision models, the behavior of these systems must be explicable to ensure confidence in their reliability and fairness. A common approach
    to evaluate deep learning models is to build a labeled test
    set with attributes of interest and assess how well it performs. However, creating a balanced test set (i.e., one that
    is uniformly sampled over all the important traits) is often time-consuming, expensive, prone to mistakes, and ultimately impractical. The question we try to address is: can
    we evaluate the sensitivity of deep learning models to arbitrary visual attributes without a test set?
    This paper argues the case that Zero-shot Model Diagnosis (ZOOM) is possible without the need for a test set nor
    labeling. To avoid the need for test sets, our system relies on
    the CLIP and a generative model (e.g., StyleGAN). The key
    idea is enabling the user to select a set of prompts (relevant
    to the problem) and our system will automatically search
    for semantic counterfactual images (i.e., synthesized images
    that flip the prediction in the case of a binary classifier) using the generative model. We use two tasks, attribute classification and key-point detection, in two visual domains (human faces and animal faces) to demonstrate the viability of
    our methodology. Extensive experiments demonstrate that
    our method is capable of producing counterfactual images
    and offering sensitivity analysis for model diagnosis without the need for a test set.</p>
</div>
<!-- <div class="content">
  <h2>Background</h2>
  <p> Given a particular subject such as clock (shown in the real images on the left), it is very challenging to generate it in different contexts with state-of-the-art text-to-image models, while maintaining high fidelity to its key visual features. Even with dozens of iterations over a text prompt that contains a detailed description of the appearance of the clock (<em>"retro style yellow alarm clock with a white clock face and a yellow number three on the right part of the clock face in the jungle"</em>), the Imagen model [Saharia et al., 2022] can't reconstruct its key visual features (third column). Furthermore, even models whose text embedding lies in a shared language-vision space and can create semantic variations of the image, such as DALL-E2 [Ramesh et al., 2022], can neither reconstruct the appearance of the given subject nor modify the context (second column). In contrast, our approach (right) can synthesize the clock with high fidelity and in new contexts (<em>"a [V] clock in the jungle"</em>).</p>
  <br>
  <img class="summary-img" src="./ZOOM_files/background.png" style="width:100%;"> <br>
</div> -->
<div class="content">
  <h2>Approach</h2>
  <br>
  <img class="summary-img" src="./ZOOM_files/overall_pipeline.jpg" style="width:100%;"> <br>
  <p> The ZOOM framework. Black solid lines stand for forward passes, red dashed lines stand for backpropagation, and purple
    dashed lines stands for inference after the optimization converges. The user inputs single or multiple attributes, and we map them into
    edit directions. Then we assign to each edit direction (attribute) a weight, which represents how much we are adding/removing this attribute. We iteratively perform adversarial learning on the attribute space to maximize the counterfactual
    effectiveness. More details in the paper. </p>
</div>
<div class="content">
  <h2>Edit Visualization</h2>
  <p></p>
  <h3>Multi-attribute counterfactual in the human face domain</h3>
  <img class="summary-img" src="./ZOOM_files/multi_attr_human.jpg" style="width:100%;"> <br>
  <h3>Single  attribute progressive edits</h3>
  <img class="summary-img-l" src="./ZOOM_files/single_attr_dog.jpg" style="width:48%;">
  <img class="summary-img-r" src="./ZOOM_files/single_attr_young.jpg" style="width:48%;"> <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <br>
  <p></p>
  <br>
</div>
<div class="content">
  <h2>Histogram Results</h2>
  <h3>Results for diagnosing sensitivities of different models </h3>
<img class="summary-img" src="./ZOOM_files/histograms.png" style="width:100%;"><br>
  <h3>Results for diagnosing with 19 attributes </h3>
<img class="summary-img" src="./ZOOM_files/grand_histogram.jpg" style="width:100%;"><br>
<h3>Results for diagnosing with attribute combinations </h3>
<img class="summary-img" src="./ZOOM_files/multi_attr_histogram.jpg" style="width:100%;"><br>
</div>
<!-- <div class="content">
  <h2>Text-Guided View Synthesis</h2>
  <p>Our technique can synthesized images with specified viewpoints for a subject cat (left to right: top, bottom, side and back views). Note that the generated poses are  different from the input poses, and the background changes in a realistic manner given a pose change. We also highlight the preservation of complex fur patterns on the subject cat's forehead.</p>
  <br>
  <img class="summary-img" src="./ZOOM_files/novel_views.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Property Modification</h2>
  <p>We show color modifications in the first row (using prompts ``a [color] [V] car''), and crosses between a specific dog and different animals in the second row (using prompts ``a cross of a [V] dog and a [target species]''). We highlight the fact that our method preserves unique visual features that give the subject its identity or essence, while performing the required property modification.</p>
  <br>
  <img class="summary-img" src="./ZOOM_files/property_modification.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Accessorization</h2>
  <p>Outfitting a dog with accessories. The identity of the subject is preserved and many different outfits or accessories can be applied to the dog given a prompt of type <em>"a [V] dog wearing a police/chef/witch outfit''</em>. We observe a realistic interaction between the subject dog and the outfits or accessories, as well as a large variety of possible options.</p>
  <br>
  <img class="summary-img" src="./ZOOM_files/accessories.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>BibTex</h2>
  <code> @article{ruiz2022dreambooth,<br>
  &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion Models for Subject-Driven Generation},<br>
  &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br>
  &nbsp;&nbsp;year={2022}<br>
  } </code> 
</div>
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or for their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained models of Imagen. Finally, a special thank you to David Salesin for his feedback, advice and for his support for the project.
    <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). --> 
  <!-- </p> -->

  <div class="content">
    <h2>BibTex</h2>
    <code> @inproceedings{zoom2023cvpr,<br>
    &nbsp;&nbsp;title={ZOOM: Zero-shot Model Diagnosis },<br>
    &nbsp;&nbsp;author={Jinqi Luo and Zhaoning Wang and Chen Henry Wu and Dong Huang and Fernando De La Torre},<br>
    &nbsp;&nbsp;booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},<br>
    &nbsp;&nbsp;year={2023}<br>
    } </code> 
  </div>
</body>
</html>